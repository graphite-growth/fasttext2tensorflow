{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FastText Inference Process\n",
        "\n",
        "In this notebook we will analyze the inner workings of a FastText supervised model when making inferences. We will replicate what the trained model does in plain Python so it's easier to understand. The process can be split into the following steps:\n",
        "\n",
        "1. Getting an embedding for the given sentence.\n",
        "2. Computing the inferred class given this embedding.\n",
        "\n",
        "First we train a simple model."
      ],
      "metadata": {
        "id": "15B6ZpW8pzxX"
      },
      "id": "15B6ZpW8pzxX"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install fasttext\n",
        "! wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz && tar xvzf cooking.stackexchange.tar.gz\n",
        "! head -n 12404 cooking.stackexchange.txt > cooking.train\n",
        "! tail -n 3000 cooking.stackexchange.txt > cooking.valid"
      ],
      "metadata": {
        "id": "s0cQuf1mWrc7"
      },
      "id": "s0cQuf1mWrc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwz2HIXdXdNZ",
        "outputId": "ff23935f-21d9-44bd-f874-a0489a4c4c0f"
      },
      "id": "uwz2HIXdXdNZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cooking.stackexchange.id\tcooking.stackexchange.txt  readme.txt\n",
            "cooking.stackexchange.tar.gz\tcooking.train\t\t   sample_data\n",
            "cooking.stackexchange.tar.gz.1\tcooking.valid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aead067",
      "metadata": {
        "id": "3aead067"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import fasttext\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR=1.0 \n",
        "EPOCH=30 \n",
        "WORD_NGRAMS=2 \n",
        "BUCKET=200000\n",
        "DIM=16\n",
        "MINN=3\n",
        "MAXN=6"
      ],
      "metadata": {
        "id": "Dnhwa7AHYJt9"
      },
      "id": "Dnhwa7AHYJt9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "977db69c",
      "metadata": {
        "id": "977db69c"
      },
      "outputs": [],
      "source": [
        "model = fasttext.train_supervised(input=\"cooking.train\", lr=LR, epoch=EPOCH, wordNgrams=WORD_NGRAMS, \n",
        "                                  bucket=BUCKET, dim=DIM, minn=MINN, maxn=MAXN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d2df131",
      "metadata": {
        "id": "6d2df131"
      },
      "source": [
        "# Get FastText parameters\n",
        "\n",
        "Now we get the parameters and hyperparameters from the FastText binary model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "227a193c",
      "metadata": {
        "id": "227a193c",
        "outputId": "c0af27a3-5568-4b74-f6b4-e3c3429ae6fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 14543\n",
            "Number of labels: 735\n",
            "Embedding dimension: 16\n",
            "Bucket size: 200000\n",
            "Minn: 3\n",
            "Maxn: 6\n",
            "wordNgrams max length: 2\n",
            "Input matrix shape: (214543, 16)\n",
            "Output matrix shape: (735, 16)\n"
          ]
        }
      ],
      "source": [
        "vocabulary = model.get_words()\n",
        "labels = model.labels\n",
        "embedding_dim = model.get_dimension()\n",
        "bucket_size = model.f.getArgs().bucket\n",
        "minn = model.f.getArgs().minn\n",
        "maxn = model.f.getArgs().maxn\n",
        "wordNgrams = model.f.getArgs().wordNgrams\n",
        "input_matrix = model.get_input_matrix()\n",
        "output_matrix = model.get_output_matrix()\n",
        "\n",
        "print(\"Vocabulary size:\", len(vocabulary))\n",
        "print(\"Number of labels:\", len(labels))\n",
        "print(\"Embedding dimension:\", embedding_dim)\n",
        "print(\"Bucket size:\", bucket_size)\n",
        "print(\"Minn:\", minn)\n",
        "print(\"Maxn:\", maxn)\n",
        "print(\"wordNgrams max length:\", wordNgrams)\n",
        "print(\"Input matrix shape:\", input_matrix.shape)\n",
        "print(\"Output matrix shape:\", output_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "082761ef",
      "metadata": {
        "id": "082761ef"
      },
      "source": [
        "# 1. Compute full word vectors\n",
        "\n",
        "Full word vectors are stored in the first chunk of the input_matrix. These are words that show up in the training data at least **minCount** times. If there are no character n-grams then this vector will be the final word vector. If there are character n-grams then the final word vector is the mean of the word vector and the subword vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972ef6e1",
      "metadata": {
        "id": "972ef6e1",
        "outputId": "f12c571f-5dca-4904-c26d-54663595bdbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "957"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "word = \"meatballs\"\n",
        "idx_a = model.get_words().index(word)\n",
        "idx_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6dd56fa",
      "metadata": {
        "id": "c6dd56fa",
        "outputId": "edee3684-1ca7-4550-e9a8-9aac5cb1f34b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "957"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "idx_b = model.get_word_id(word)\n",
        "idx_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41f26ad",
      "metadata": {
        "id": "b41f26ad",
        "outputId": "afac875f-1d83-4bbc-9b3b-61687027d2b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.26651105,  0.02829988, -0.18690547,  0.31578544, -0.4148955 ,\n",
              "        0.71081346,  0.8592059 , -0.30728486, -0.15301947, -0.59633934,\n",
              "        0.45763463, -0.22829835, -0.14477722,  0.29369563, -0.02462302,\n",
              "        0.21090278], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "input_matrix[idx_a]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82a10d57",
      "metadata": {
        "id": "82a10d57"
      },
      "source": [
        "# 2. Compute all possible subwords\n",
        "\n",
        "FastText adds the \"Beginning of Word\" and \"End of Word\" characteres (< and >) to the words before computing character n-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa43b41f",
      "metadata": {
        "id": "aa43b41f"
      },
      "outputs": [],
      "source": [
        "def get_subwords(word: str, minn: int, maxn: int) -> List[str]:\n",
        "    word = '<' + word + '>' \n",
        "    subwords = set(\n",
        "        [\n",
        "            word[i:i+size]\n",
        "            for size in range(minn, maxn+1)\n",
        "            for i in range(len(word)-size+1)\n",
        "        ]\n",
        "    )\n",
        "    return list(subwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41f46d7",
      "metadata": {
        "id": "a41f46d7",
        "outputId": "37e38f92-20e1-430f-83b6-41674aa043d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['eatba',\n",
              " '<me',\n",
              " 'meat',\n",
              " 'tbal',\n",
              " 'meatba',\n",
              " 'tball',\n",
              " 'atba',\n",
              " 'tba',\n",
              " 'bal',\n",
              " 'alls',\n",
              " 'eat',\n",
              " 'ls>',\n",
              " 'alls>',\n",
              " 'balls',\n",
              " 'eatb',\n",
              " '<mea',\n",
              " 'ball',\n",
              " 'tballs',\n",
              " 'eatbal',\n",
              " 'balls>',\n",
              " '<meatb',\n",
              " 'all',\n",
              " 'mea',\n",
              " 'lls>',\n",
              " '<meat',\n",
              " 'meatb',\n",
              " 'atball',\n",
              " 'atbal',\n",
              " 'atb',\n",
              " 'lls']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "get_subwords(\"meatballs\", minn=minn, maxn=maxn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b1a32e8",
      "metadata": {
        "id": "2b1a32e8"
      },
      "source": [
        "# 3. Find subword vectors in input matrix\n",
        "\n",
        "The input matrix is of shape **(n_vocabulary + bucket_size, dim)**. For every unique word in the vocabulary there is a corresponding vector in the first chunk of the matrix. After this the remaining chunk (of bucket_size) holds vectors for subword and word n-grams, which might collide since the bucket size is fixed, as opposed to word vectors which do not collide.\n",
        "\n",
        "To find the n-gram index in the matrix we have to do the following procedure:\n",
        "\n",
        "1. Hash the subword string characters into a single integer.\n",
        "2. Modulo this number with the bucket size, which will map the number in one of the bucket slots.\n",
        "3. Add the vocabulary length to the previous result to get the index of the subword in the input_matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62587aa3",
      "metadata": {
        "id": "62587aa3"
      },
      "outputs": [],
      "source": [
        "def get_hash(subword: str) -> int:\n",
        "    h = 2166136261\n",
        "    for c in subword:\n",
        "        c = ord(c) % 2**8\n",
        "        h = (h ^ c) % 2**32\n",
        "        h = (h * 16777619) % 2**32\n",
        "    return h\n",
        "\n",
        "def get_subword_index(subword, bucket, nb_words):\n",
        "    return (get_hash(subword) % bucket) + nb_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cf2cd08",
      "metadata": {
        "id": "6cf2cd08"
      },
      "outputs": [],
      "source": [
        "# Equivalent implementation\n",
        "def get_hash(subword: str) -> np.uint32:\n",
        "    h = np.uint32(2166136261)\n",
        "    for c in subword:\n",
        "        c = np.uint32(np.int8(ord(c)))\n",
        "        h = np.uint32(h ^ c)\n",
        "        h = np.uint32(h * 16777619)\n",
        "    return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "674f05c5",
      "metadata": {
        "id": "674f05c5",
        "outputId": "421ae160-aeeb-4a48-cb94-cd86c76977c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16148"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "idx_a = get_subword_index('<me', bucket_size, len(vocabulary))\n",
        "idx_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295f4d2c",
      "metadata": {
        "id": "295f4d2c",
        "outputId": "a6c594d9-3dca-46d8-d5f5-b58aa9712788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16148"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "idx_b = model.get_subword_id('<me')\n",
        "idx_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff1e0cb",
      "metadata": {
        "id": "0ff1e0cb",
        "outputId": "b45726fe-d3ff-4bc5-95b3-dba27f72ac79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.15881912, -2.008248  , -2.0042837 , -0.27310666, -1.9667645 ,\n",
              "       -1.5717009 ,  1.1889542 , -0.9687559 ,  1.5461129 , -0.14315978,\n",
              "        0.75167054, -0.01499958,  0.61466074, -0.5986607 ,  0.49294454,\n",
              "       -0.610307  ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "input_matrix[idx_a]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14758fc8",
      "metadata": {
        "id": "14758fc8"
      },
      "source": [
        "# 4. Find wordNgram features in the input matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compute the wordNgrams for a particular sentence you need to: \n",
        "1. Get the hashes for the words in the sentence, including the EOS token.\n",
        "2. Make ngrams from size 2 up to the max ngram size (**wordNgrams** parameter)\n",
        "3. Get a single index (feature) per ngram by recursively hashing (again but different hashing function) each word hash integer into a single integer.\n",
        "\n",
        "Suppose we want to get the wordNgram features for the sentence: **what is pizza?**"
      ],
      "metadata": {
        "id": "e_rVnLU1yEQv"
      },
      "id": "e_rVnLU1yEQv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Get hashes for words in the sentence\n",
        "In this case we will get the hashes for the following words:\n",
        "1. what\n",
        "2. is\n",
        "3. pizza?\n",
        "4. <\\/s>\n",
        "\n",
        "To do this we will use the previously defined `get_hash` function for characters."
      ],
      "metadata": {
        "id": "K-NYEPFY9IVf"
      },
      "id": "K-NYEPFY9IVf"
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"what is pizza?\"\n",
        "# Append EOL token\n",
        "sentence += \" </s>\"\n",
        "# Get hashes for each word\n",
        "words = sentence.split()\n",
        "hashes = [np.int32(get_hash(word)) for word in words]\n",
        "\n",
        "\n",
        "hashes, words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z1RxtbR8ZYv",
        "outputId": "37d2ee1c-6372-4908-a6c6-17db4273ae6c"
      },
      "id": "6Z1RxtbR8ZYv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([-2056350673, 1312329493, 378301358, -677604519],\n",
              " ['what', 'is', 'pizza?', '</s>'])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Make ngrams from size 2 up to the max ngram size (wordNgrams parameter)\n",
        "\n",
        "Now we have to generate the word ngrams (only if wordNgrams > 1). Word Ngrams are generated for each length from 2 up to the wordNgrams parameter or \\[2, wordNgrams+1\\].\n",
        "\n",
        "As we already have a list of hashes instead of a sentence with words, we generate the ngrams using the hashes directly instead of doing it with words and then hashing.\n",
        "  "
      ],
      "metadata": {
        "id": "biHWbJgi-Ij5"
      },
      "id": "biHWbJgi-Ij5"
    },
    {
      "cell_type": "code",
      "source": [
        "wordngrams = [\n",
        "    words[i:i+size]\n",
        "    for size in range(2, wordNgrams+1)\n",
        "    for i in range(len(words)-size+1)\n",
        "]\n",
        "wordngrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XBBmTTzp-W4",
        "outputId": "fe0c24d6-59f2-4457-c849-d68c9ce563fd"
      },
      "id": "-XBBmTTzp-W4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['what', 'is'], ['is', 'pizza?'], ['pizza?', '</s>']]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordngrams = [\n",
        "    hashes[i:i+size]\n",
        "    for size in range(2, wordNgrams+1)\n",
        "    for i in range(len(hashes)-size+1)\n",
        "]\n",
        "wordngrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhARvlVEpuFS",
        "outputId": "a83fd0af-3a63-4409-d209-60e08178d48a"
      },
      "id": "QhARvlVEpuFS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-2056350673, 1312329493], [1312329493, 378301358], [378301358, -677604519]]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Get a single index per ngram by recursively hashing each word hash integer into a single integer.\n",
        "\n",
        "We want to combine each ngram's hashes (words) into a single index for the input_matrix. A different recursive hashing function is used than the one for hashing string characters. After hashes are obtained we pass through modulo % buckets and add the vocabulary length to find the actual index in the input matrix."
      ],
      "metadata": {
        "id": "xuX6Q1UIqevF"
      },
      "id": "xuX6Q1UIqevF"
    },
    {
      "cell_type": "code",
      "source": [
        "wordngrams_indexes = list()\n",
        "\n",
        "for ngram in wordngrams:\n",
        "    h = ngram[0]\n",
        "    for word_hash in ngram[1:]:\n",
        "        h = np.uint64(h * 116049371 + word_hash)\n",
        "    ngram_index = len(vocabulary) + int(h) % bucket_size\n",
        "    wordngrams_indexes.append(ngram_index)\n",
        "wordngrams_indexes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vygWdZ5KqY9F",
        "outputId": "35432f58-2a5e-4d41-89eb-25657a1d019f"
      },
      "id": "vygWdZ5KqY9F",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[18969, 114804, 155842]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We end up with the indices corresponding to the following 3 wordNgram features.\n",
        "\n",
        "1. \"what is\"\n",
        "2. \"is pizza\" \n",
        "3. \"pizza <\\/s>\"\n",
        "\n",
        "It's not as easy to verify the right index for a given wordngram using the Python model as it is with words and subwords, thanks to **model.get_word_id()** and **model.get_subword_id()**, there is not a **model.get_wordngram_id()** available to verify. So to check these are in fact the right indexes we have to modify the original source to print them which is not included in this notebook.\n",
        "\n",
        "Once we have the wordNgram indexes we can look them up in the input matrix to retrieve the embeddings."
      ],
      "metadata": {
        "id": "kNxf4IXTtqsK"
      },
      "id": "kNxf4IXTtqsK"
    },
    {
      "cell_type": "code",
      "source": [
        "features = [input_matrix[word_ngram_id] for word_ngram_id in wordngrams_indexes]\n",
        "features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuJjgt6utrZq",
        "outputId": "e71c1854-2757-43b6-89b0-1536b2d93e45"
      },
      "id": "YuJjgt6utrZq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-0.5212617 , -0.15074034, -0.80975354, -0.76292604, -0.16668485,\n",
              "         1.5814468 , -0.87663573, -0.2633228 ,  0.16226742, -0.5250907 ,\n",
              "         0.26750985, -0.05091313,  0.40993208,  0.6963279 , -0.5980508 ,\n",
              "        -1.5497122 ], dtype=float32),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       dtype=float32),\n",
              " array([-0.4779425 ,  0.1734612 ,  0.46179655, -0.07976016,  0.30360892,\n",
              "        -0.28149533,  0.42330447, -0.22358117, -0.61802363, -0.10678729,\n",
              "        -0.01147463,  0.13381556,  1.1937575 ,  0.8168405 ,  0.01947787,\n",
              "         0.72266304], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actual FastText implementation\n",
        "\n",
        "The previous implementantion makes it easier to understand the steps involved, however in the original FastText C++ source, getting the wordNgram indexes for a given sentence is computed by generating and hashing the ngrams to a single list in one go. The following definition is a closer replica of the FastText process."
      ],
      "metadata": {
        "id": "k2UH7Bqtsete"
      },
      "id": "k2UH7Bqtsete"
    },
    {
      "cell_type": "code",
      "source": [
        "def add_word_ngram(line: list, hashes: list, n: int, bucket: int, nwords: int) -> List[int]:\n",
        "    for i in range(len(hashes)):\n",
        "        h = np.int32(hashes[i])\n",
        "        j = i + 1\n",
        "        while j < len(hashes) and j < i + n:\n",
        "            h = np.uint64(h * 116049371 + np.int32(hashes[j]))\n",
        "            line.append(nwords + (int(h) % bucket))\n",
        "            j += 1\n",
        "    return line"
      ],
      "metadata": {
        "id": "rGTRkR7w-HSl"
      },
      "id": "rGTRkR7w-HSl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_ngrams_idxs = add_word_ngram(line=list(), hashes=hashes, n=wordNgrams, bucket=bucket_size, nwords=len(vocabulary))\n",
        "word_ngrams_idxs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3948YIC-7-u",
        "outputId": "f668dd6d-62be-4f6a-963b-a88adef5a1d1"
      },
      "id": "D3948YIC-7-u",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[18969, 114804, 155842]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d019149",
      "metadata": {
        "id": "6d019149"
      },
      "source": [
        "# 5. Replicating FastText vector functions\n",
        "\n",
        "## 5.1 model.get_word_vector(string)\n",
        "\n",
        "In the case of minn and maxn equal to zero (no character n-grams), the full word index is the final word vector. However, when we use character n-grams the final word vector is the mean of the full word vector and the compounding subword vectors. Notice that when computing sentence vectors we do not make use of model.get_word_vector(), since averaging all features at once (subwords, words, wordngrams) might produce a different result that if we average at word level and then average those again for a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c4a6207",
      "metadata": {
        "id": "6c4a6207"
      },
      "outputs": [],
      "source": [
        "def get_word_vector(word: str) -> np.ndarray:\n",
        "    subwords = get_subwords(word, minn, maxn)\n",
        "    subword_idxs = [get_subword_index(subword, bucket_size, len(vocabulary)) for subword in subwords]\n",
        "    embeddings = [input_matrix[idx] for idx in subword_idxs]\n",
        "    if word in vocabulary:\n",
        "        word_idx = vocabulary.index(word)\n",
        "        embeddings += [input_matrix[word_idx]]\n",
        "    return np.mean(embeddings, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c12c816",
      "metadata": {
        "id": "5c12c816",
        "outputId": "42bec295-4e90-4d44-8e5d-9e9571d22506",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.6272257 ,  0.09336659, -0.27236226,  0.40666848, -0.59998316,\n",
              "        0.13910004,  1.4206132 , -0.4222818 ,  0.2345608 , -0.67761075,\n",
              "        0.09557244, -0.32499897, -0.12342142,  0.38488638,  0.21018639,\n",
              "       -0.03817398], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "get_word_vector(\"meatballs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c6a3d0",
      "metadata": {
        "id": "a0c6a3d0",
        "outputId": "0acdbf1c-4df7-406c-92a6-106c805ffcbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.62722564,  0.09336659, -0.27236217,  0.40666845, -0.5999832 ,\n",
              "        0.13910004,  1.4206134 , -0.4222818 ,  0.23456082, -0.67761075,\n",
              "        0.09557243, -0.32499892, -0.12342141,  0.38488638,  0.21018638,\n",
              "       -0.03817399], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "model.get_word_vector(\"meatballs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bd74d1c",
      "metadata": {
        "id": "4bd74d1c",
        "outputId": "5014ed85-ce4f-4fb5-915e-7c060a35d610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "np.isclose(\n",
        "    get_word_vector(\"meatballs\"),\n",
        "    model.get_word_vector(\"meatballs\")\n",
        ").all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "333da734",
      "metadata": {
        "id": "333da734"
      },
      "source": [
        "## 5.2 model.get_sentence_vector(string)\n",
        "\n",
        "The sentence vector is the mean of all feature embeddings, these are all the individual full word embeddings, all the subword embeddings and all wordNgram embeddings. It also includes the End Of Sentence token as if it were a single word."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_features(words: List[str], input_matrix: np.ndarray, vocabulary: List[str]):\n",
        "    \"\"\"\n",
        "    Returns a list of all individual word features, 1 per word if word is in vocabulary\n",
        "    \"\"\"\n",
        "    features = [input_matrix[vocabulary.index(word)] for word in words if word in vocabulary]\n",
        "    return features\n",
        "\n",
        "\n",
        "def get_subword_features(words: List[str], input_matrix: np.ndarray, vocabulary: List[str], minn: int, maxn: int, bucket_size: int) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns a list of all subword features, all the variable number of subword features\n",
        "    from each word are flattened together in a single list\n",
        "    \"\"\"\n",
        "    \n",
        "    def get_subwords(word: str, minn: int, maxn: int) -> List[str]:\n",
        "        word = '<' + word + '>' \n",
        "        subwords = set(\n",
        "            [\n",
        "                word[i:i+size]\n",
        "                for size in range(minn, maxn+1)\n",
        "                for i in range(len(word)-size+1)\n",
        "            ]\n",
        "        )\n",
        "        return list(subwords)\n",
        "\n",
        "    def get_hash(subword: str) -> int:\n",
        "        h = 2166136261\n",
        "        for c in subword:\n",
        "            c = ord(c) % 2**8\n",
        "            h = (h ^ c) % 2**32\n",
        "            h = (h * 16777619) % 2**32\n",
        "        return h\n",
        "\n",
        "    def get_subword_index(subword: str, bucket: int, nb_words: int):\n",
        "        return (get_hash(subword) % bucket) + nb_words\n",
        "\n",
        "    \n",
        "    features = list()\n",
        "    for word in words:\n",
        "        if word == \"</s>\":\n",
        "          continue\n",
        "        subwords = get_subwords(word, minn, maxn)\n",
        "        subword_idxs = [get_subword_index(subword, bucket_size, len(vocabulary)) for subword in subwords]\n",
        "        subword_embeddings = [input_matrix[idx] for idx in subword_idxs]\n",
        "        features += subword_embeddings\n",
        "    return features\n",
        "\n",
        "\n",
        "def get_wordNgram_features(words: List[str], input_matrix: np.ndarray, nwords: int, max_ngram_length: int, bucket: int) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns a list of all wordNgram features according to size n\n",
        "    \"\"\"\n",
        "    def get_hash(subword: str) -> int:\n",
        "        h = 2166136261\n",
        "        for c in subword:\n",
        "            c = ord(c) % 2**8\n",
        "            h = (h ^ c) % 2**32\n",
        "            h = (h * 16777619) % 2**32\n",
        "        return h\n",
        "  \n",
        "    def get_wordngram_index(hashes: list, bucket: int, nwords: int) -> int:\n",
        "        h = hashes[0]\n",
        "        for word_hash in hashes[1:]:\n",
        "            h = np.uint64(h * 116049371 + word_hash)\n",
        "        return (nwords + int(h) % bucket)\n",
        "\n",
        "    def get_wordngrams(words: List[str], max_ngram_length: int) -> List[np.ndarray]:\n",
        "        hashes = [np.int32(get_hash(word)) for word in words]\n",
        "        wordngrams = [\n",
        "            hashes[i:i+size]\n",
        "            for size in range(2, max_ngram_length+1)\n",
        "            for i in range(len(hashes)-size+1)\n",
        "        ]\n",
        "        return wordngrams\n",
        "\n",
        "    ngrams = get_wordngrams(words, max_ngram_length)\n",
        "    word_ngrams_idxs = [get_wordngram_index(ngram, bucket, nwords) for ngram in ngrams]\n",
        "    features = [input_matrix[word_ngram_id] for word_ngram_id in word_ngrams_idxs]\n",
        "    \n",
        "    return features"
      ],
      "metadata": {
        "id": "FSjPU0SEb3zF"
      },
      "id": "FSjPU0SEb3zF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_vector(sentence: str, minn: int, maxn: int, wordNgrams: int, bucket_size: int, vocabulary: List[str], input_matrix: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Equivalent to model.get_sentence_vector()\n",
        "    \"\"\"\n",
        "    sentence += \" </s>\"\n",
        "    words = sentence.split()\n",
        "    feature_embeddings = []\n",
        "    \n",
        "    # Get word features\n",
        "    feature_embeddings += get_word_features(words, input_matrix, vocabulary)\n",
        "    \n",
        "    # Get subword features\n",
        "    if minn > 0 and maxn > 0:\n",
        "        feature_embeddings += get_subword_features(words, input_matrix, vocabulary, minn, maxn, bucket_size)\n",
        "\n",
        "    # Get wordNgram features\n",
        "    if wordNgrams > 1:\n",
        "        feature_embeddings += get_wordNgram_features(words, input_matrix, len(vocabulary), \n",
        "                                                     wordNgrams, bucket_size)\n",
        "    \n",
        "    # Compute mean of all features\n",
        "    sentence_vec = np.mean(feature_embeddings, axis=0)\n",
        "    return sentence_vec"
      ],
      "metadata": {
        "id": "E7-WsYfYbzTt"
      },
      "id": "E7-WsYfYbzTt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "293f555f",
      "metadata": {
        "id": "293f555f",
        "outputId": "0895869c-3c18-4150-fae0-fef82662dce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# Compare against model generated sentence vector\n",
        "sentence = \"How to make a pepperoni pizza?\"\n",
        "np.isclose(\n",
        "    get_sentence_vector(sentence, minn, maxn, wordNgrams, bucket_size, vocabulary, input_matrix),\n",
        "    model.get_sentence_vector(sentence)\n",
        ").all()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentence_vector(sentence, minn, maxn, wordNgrams, bucket_size, vocabulary, input_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVOncL3UeiWb",
        "outputId": "519879ed-ed6f-4453-83a8-b0434643c0b5"
      },
      "id": "cVOncL3UeiWb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.03875927,  0.05721441,  0.19570303,  0.00276239,  0.25914463,\n",
              "       -0.51486284, -0.09010237,  0.0232471 , -0.2304556 , -0.02440632,\n",
              "        0.17151904, -0.05499319,  0.07283257,  0.3178859 ,  0.15399942,\n",
              "       -0.19516972], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_sentence_vector(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrOUTW6KeksM",
        "outputId": "b9827170-c55f-4204-b180-e0a469bde830"
      },
      "id": "DrOUTW6KeksM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.03875926,  0.05721441,  0.1957031 ,  0.00276241,  0.25914463,\n",
              "       -0.5148627 , -0.09010238,  0.02324709, -0.2304556 , -0.02440632,\n",
              "        0.171519  , -0.05499319,  0.07283266,  0.3178859 ,  0.15399945,\n",
              "       -0.19516975], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85beeac5",
      "metadata": {
        "id": "85beeac5"
      },
      "source": [
        "# 5. Compute predictions using output_matrix\n",
        "\n",
        "**\\*Assumes model uses a softmax loss**\n",
        "\n",
        "The supervised model FastText uses is a single layer neural network. Once we have a sentence vector we can obtain the output class vector by matrix multiplying the output_matrix (n_classes, dim) and the sentence vector (dim,), which will result in an output vector with shape (n_classes,) which has a value for each possible class. These scores are then softmaxed to convert to probabilities and then the highest one is selected for prediction.\n",
        "\n",
        "In this notebook we are assuming that we are using the default softmax loss when training the supervised model. To implement a different loss function we need to define it as a function and apply it to the output vector instead of **softmax** before selecting the prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0SKjv7YBp6t",
        "outputId": "c364eebb-b0d0-4bd4-e558-55572ff71084"
      },
      "id": "E0SKjv7YBp6t",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(735, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5f68e3e",
      "metadata": {
        "id": "d5f68e3e"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
        "    return f_x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def exp_normalize(x):\n",
        "    b = x.max()\n",
        "    y = np.exp(x - b)\n",
        "    return y / y.sum()"
      ],
      "metadata": {
        "id": "48xIhYyML9ai"
      },
      "id": "48xIhYyML9ai",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "521c1b38",
      "metadata": {
        "id": "521c1b38"
      },
      "outputs": [],
      "source": [
        "def get_prediction(sentence: str, minn: int, maxn: int, wordNgrams: int, bucket_size: int, vocabulary: List[str], input_matrix: np.ndarray):\n",
        "    sentence_vec = get_sentence_vector(sentence, minn, maxn, wordNgrams, bucket_size, vocabulary, input_matrix)\n",
        "    pred_idx = np.argmax(softmax(np.matmul(output_matrix, sentence_vec)))\n",
        "    pred_proba = np.max(softmax(np.matmul(output_matrix, sentence_vec)))\n",
        "    pred_label = labels[pred_idx]\n",
        "    return pred_label, pred_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f51d3bd",
      "metadata": {
        "id": "8f51d3bd",
        "outputId": "7132ea7c-17fa-4625-eb11-1c921b7ea5ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('__label__baking', 0.6910467)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "get_prediction(\"Italian food recipes\", minn, maxn, wordNgrams, bucket_size, vocabulary, input_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aab26456",
      "metadata": {
        "id": "aab26456",
        "outputId": "bb0c07a5-2f65-4693-c561-98d3193e6683",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('__label__baking',), array([0.6910578]))"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "model.predict(\"Italian food recipes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f163cbe",
      "metadata": {
        "id": "2f163cbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53257ad6-f6f0-4c55-be10-7a0a442d2982"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "np.isclose(get_sentence_vector(sentence, minn, maxn, wordNgrams, bucket_size, vocabulary, input_matrix), model.get_sentence_vector(sentence)).all()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "G7Al2ZxHvqxm"
      },
      "id": "G7Al2ZxHvqxm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "fasttext-inference-reverse-engineering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}